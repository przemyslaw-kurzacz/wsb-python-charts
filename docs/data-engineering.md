# üìä Data Engineering - Przetwarzanie i czyszczenie danych

> Dokumentacja techniczna procesu in≈ºynierii danych w projekcie Data Charts

**Data:** 28 stycznia 2026  
**Wersja:** 1.0

---

## üéØ Cel dokumentu

Ten dokument opisuje **jak przetwarzamy i czy≈õcimy dane CSV** w naszej aplikacji:
- Jakie problemy rozwiƒÖzujemy
- Jakie techniki stosujemy
- Jakie biblioteki u≈ºywamy
- Jak to dzia≈Ça pod maskƒÖ

---

## üìã Spis tre≈õci

1. [Pipeline przetwarzania danych](#-pipeline-przetwarzania-danych)
2. [Etap 1: Upload i walidacja](#-etap-1-upload-i-walidacja)
3. [Etap 2: Detekcja formatu](#-etap-2-detekcja-formatu)
4. [Etap 3: Parsowanie](#-etap-3-parsowanie)
5. [Etap 4: Czyszczenie danych](#-etap-4-czyszczenie-danych)
6. [Etap 5: Profilowanie kolumn](#-etap-5-profilowanie-kolumn)
7. [Etap 6: Przygotowanie do wizualizacji](#-etap-6-przygotowanie-do-wizualizacji)
8. [Problemy i rozwiƒÖzania](#-problemy-i-rozwiƒÖzania)
9. [Przyk≈Çady](#-przyk≈Çady)

---

## üîÑ Pipeline przetwarzania danych

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. UPLOAD                                                  ‚îÇ
‚îÇ  U≈ºytkownik przesy≈Ça plik CSV przez formularz              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  2. WALIDACJA                                               ‚îÇ
‚îÇ  - Rozmiar pliku (max 16MB)                                ‚îÇ
‚îÇ  - Rozszerzenie (.csv)                                      ‚îÇ
‚îÇ  - Plik nie jest pusty                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  3. DETEKCJA FORMATU                                        ‚îÇ
‚îÇ  - Encoding (UTF-8, UTF-8-BOM, CP1250)                     ‚îÇ
‚îÇ  - Delimiter (przecinek, ≈õrednik, tab)                      ‚îÇ
‚îÇ  - Czy ma nag≈Ç√≥wki                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  4. PARSOWANIE                                              ‚îÇ
‚îÇ  - pandas.read_csv() z wykrytymi parametrami               ‚îÇ
‚îÇ  - Obs≈Çuga b≈Çƒôd√≥w kodowania                                 ‚îÇ
‚îÇ  - Obs≈Çuga duplikat√≥w kolumn                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  5. CZYSZCZENIE DANYCH                                      ‚îÇ
‚îÇ  - Normalizacja pustych warto≈õci (‚Üí pd.NA)                 ‚îÇ
‚îÇ  - Konwersja typ√≥w (tekst ‚Üí liczba)                        ‚îÇ
‚îÇ  - Detekcja dat                                             ‚îÇ
‚îÇ  - Uzupe≈Çnianie brak√≥w (mediana/tekst)                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  6. PROFILOWANIE                                            ‚îÇ
‚îÇ  - Typy kolumn (numeric, categorical, datetime)            ‚îÇ
‚îÇ  - Statystyki (min, max, mean, missing)                    ‚îÇ
‚îÇ  - Sugestie do wizualizacji                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  7. PRZYGOTOWANIE DO WIZUALIZACJI                          ‚îÇ
‚îÇ  - Filtrowanie danych                                       ‚îÇ
‚îÇ  - Agregacje                                                ‚îÇ
‚îÇ  - Generowanie wykres√≥w (Plotly/Matplotlib)                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üì§ Etap 1: Upload i walidacja

### Kod: `app/main/routes.py`

```python
@bp.route('/', methods=['GET', 'POST'])
def index():
    form = UploadForm()
    
    if form.validate_on_submit():
        file = form.file.data
        
        # Walidacja rozszerzenia (WTForms)
        # FileAllowed(['csv'])
        
        # Bezpieczna nazwa
        filename = secure_filename(file.filename)
        
        # Zapisanie w folderze u≈ºytkownika
        user_folder = os.path.join(UPLOAD_FOLDER, session['username'])
        os.makedirs(user_folder, exist_ok=True)
        
        # Usuniƒôcie starych plik√≥w (tylko 1 plik per user)
        for old_file in glob.glob(os.path.join(user_folder, '*.csv')):
            os.remove(old_file)
        
        # Zapis
        filepath = os.path.join(user_folder, filename)
        file.save(filepath)
```

### Co sprawdzamy:

- ‚úÖ **Rozmiar**: max 16MB (config.py: `MAX_CONTENT_LENGTH`)
- ‚úÖ **Rozszerzenie**: tylko `.csv` (WTForms: `FileAllowed`)
- ‚úÖ **Bezpiecze≈Ñstwo**: `secure_filename()` usuwa niebezpieczne znaki
- ‚úÖ **Izolacja**: osobny folder per u≈ºytkownik

---

## üîç Etap 2: Detekcja formatu

### Kod: `app/services/csv_profile.py`

#### 2.1 Detekcja kodowania

```python
def _detect_encoding(data: bytes) -> str:
    """Wykrywa kodowanie pliku."""
    
    # UTF-8 z BOM (czƒôste w polskich CSV z Excela)
    if data.startswith(b"\xef\xbb\xbf"):
        return "utf-8-sig"
    
    # Pr√≥ba UTF-8
    try:
        data.decode("utf-8")
        return "utf-8"
    except UnicodeDecodeError:
        # Windows Polish (legacy)
        return "cp1250"
```

**Dlaczego to wa≈ºne?**
- üáµüá± Polskie znaki: ƒÖ, ƒá, ƒô, ≈Ç, ≈Ñ, √≥, ≈õ, ≈∫, ≈º
- üìä Excel na Windows czƒôsto zapisuje w CP1250
- üåç Nowoczesne systemy u≈ºywajƒÖ UTF-8

**Obs≈Çugiwane kodowania:**
1. `utf-8-sig` - UTF-8 z BOM (Excel Windows)
2. `utf-8` - Standard
3. `cp1250` - Windows Polish (legacy)

---

#### 2.2 Detekcja separatora (delimiter)

```python
def _detect_delimiter(text_sample: str) -> str:
    """Wykrywa separator kolumn."""
    
    try:
        # U≈ºyj csv.Sniffer (wbudowany w Python)
        dialect = csv.Sniffer().sniff(
            text_sample, 
            delimiters=[",", ";", "\t", "|"]
        )
        return dialect.delimiter
    except Exception:
        # Fallback: wybierz najczƒôstszy
        lines = [ln for ln in text_sample.splitlines() if ln.strip()]
        first = lines[0]
        candidates = [",", ";", "\t", "|"]
        return max(candidates, key=lambda d: first.count(d))
```

**Obs≈Çugiwane separatory:**
- `,` (przecinek) - standard miƒôdzynarodowy
- `;` (≈õrednik) - Excel PL (bo przecinek to separator dziesiƒôtny)
- `\t` (tab) - TSV files
- `|` (pipe) - rzadziej u≈ºywany

**Przyk≈Çady:**
```csv
# Przecinek
Name,Age,City
Jan,25,Warszawa

# ≈örednik
Nazwa;Wiek;Miasto
Jan;25;Warszawa

# Tab
Name    Age    City
Jan     25     Warszawa
```

---

#### 2.3 Detekcja nag≈Ç√≥wk√≥w

```python
def _has_header(text_sample: str) -> bool:
    """Sprawdza czy CSV ma wiersz nag≈Ç√≥wkowy."""
    try:
        return csv.Sniffer().has_header(text_sample)
    except Exception:
        return True  # zak≈Çadamy ≈ºe jest
```

**Dlaczego to wa≈ºne?**
- Bez nag≈Ç√≥wk√≥w trudno zrozumieƒá dane
- pandas domy≈õlnie traktuje pierwszy wiersz jako header
- Nasze wykresy wymagajƒÖ nazw kolumn

---

## üìä Etap 3: Parsowanie

### Kod: `app/main/processing.py`

```python
def parse_and_validate_csv(path: str) -> pd.DataFrame:
    """Parsuje CSV i waliduje podstawowe wymagania."""
    
    # 1. Sprawdzenie czy plik istnieje
    if not Path(path).exists():
        raise CSVValidationError("CSV file does not exist")
    
    # 2. Sprawdzenie czy nie jest pusty
    if Path(path).stat().st_size == 0:
        raise CSVValidationError("CSV file is empty")
    
    # 3. Pr√≥ba r√≥≈ºnych separator√≥w
    ALLOWED_SEPARATORS = [",", ";", "\t"]
    best_df = None
    best_cols = -1
    
    for sep in ALLOWED_SEPARATORS:
        try:
            df = pd.read_csv(path, sep=sep, engine="python")
            if df.shape[1] > best_cols:
                best_df = df
                best_cols = df.shape[1]
        except Exception:
            continue
    
    if best_df is None:
        raise CSVValidationError("Unable to parse CSV")
    
    # 4. Walidacja wymiar√≥w
    if best_df.shape[0] == 0:
        raise CSVValidationError("CSV has no rows")
    if best_df.shape[1] < 1:
        raise CSVValidationError("CSV has no columns")
    
    # 5. Czyszczenie nazw kolumn
    best_df.columns = [str(c).strip() for c in best_df.columns]
    
    return best_df
```

**Co sprawdzamy:**
- ‚úÖ Plik istnieje
- ‚úÖ Nie jest pusty
- ‚úÖ Da siƒô sparsowaƒá z kt√≥ryms z separator√≥w
- ‚úÖ Ma przynajmniej 1 wiersz i 1 kolumnƒô
- ‚úÖ Nazwy kolumn sƒÖ tekstowe

---

## üßπ Etap 4: Czyszczenie danych

### 4.1 Normalizacja pustych warto≈õci

#### Kod: `app/services/csv_profile.py`

```python
# Wczytanie z dtype=str (zachowanie wszystkich warto≈õci)
df = pd.read_csv(
    BytesIO(data),
    encoding=encoding,
    sep=delimiter,
    dtype=str,
    keep_default_na=False  # nie zastƒôpuj automatycznie
)

# Normalizacja: pusty string ‚Üí pd.NA
df = df.applymap(
    lambda x: pd.NA if (x is None or 
                        (isinstance(x, str) and x.strip() == "")) 
              else x
)
```

**Dlaczego tak?**

CSV mo≈ºe mieƒá r√≥≈ºne reprezentacje "braku danych":
```csv
Name,Age,City
Jan,25,Warszawa
Anna,,          # pusty string
Piotr,null,     # tekst "null"
Maria,NA,       # tekst "NA"
,30,Krak√≥w      # brak warto≈õci
```

**Nasza normalizacja:**
- Wszystko ‚Üí pandas `pd.NA` (unified missing value)
- U≈Çatwia p√≥≈∫niejszƒÖ detekcjƒô brak√≥w
- Kompatybilne z pandas operations

---

### 4.2 Detekcja i konwersja liczb

#### Kod: `app/services/csv_profile.py`

```python
def _try_parse_numeric(s: pd.Series) -> Tuple[Optional[pd.Series], float]:
    """Pr√≥ba konwersji na liczby. Obs≈Çuguje przecinek dziesiƒôtny."""
    
    non_null = s.dropna().astype(str).str.strip()
    
    # Normalizacja:
    # "10 000,50" ‚Üí "10000.50"
    # "10,5" ‚Üí "10.5"
    # "10.5" ‚Üí "10.5"
    normalized = (non_null
                  .str.replace(" ", "", regex=False)      # usu≈Ñ spacje
                  .str.replace(",", ".", regex=False))    # przecinek ‚Üí kropka
    
    # Konwersja
    parsed = pd.to_numeric(normalized, errors="coerce")
    success_ratio = parsed.notna().mean()
    
    # Je≈õli >90% siƒô uda≈Ço ‚Üí traktujemy jako numeric
    if success_ratio >= 0.9:
        return parsed, success_ratio
    else:
        return None, success_ratio
```

**Przyk≈Çady konwersji:**

```python
# Input (CSV)          # Output (pandas)
"10"         ‚Üí         10.0
"10,5"       ‚Üí         10.5
"10.5"       ‚Üí         10.5
"10 000"     ‚Üí         10000.0
"10 000,50"  ‚Üí         10000.5
"1.5e3"      ‚Üí         1500.0
"abc"        ‚Üí         NaN (nie da siƒô)
```

**Pr√≥g sukcesu: 90%**
- Je≈õli ‚â•90% warto≈õci da siƒô przekonwertowaƒá ‚Üí kolumna numeryczna
- Je≈õli <90% ‚Üí traktujemy jako tekst

---

### 4.3 Detekcja dat

#### Kod: `app/services/csv_profile.py`

```python
def _try_parse_datetime(s: pd.Series) -> Tuple[Optional[pd.Series], float]:
    """Pr√≥ba parsowania dat. Obs≈Çuguje formaty ISO i day-first."""
    
    non_null = s.dropna().astype(str).str.strip()
    
    # Pr√≥ba 1: ISO format (YYYY-MM-DD)
    parsed1 = pd.to_datetime(non_null, errors="coerce", utc=False)
    success1 = parsed1.notna().mean()
    
    # Pr√≥ba 2: Day-first (DD-MM-YYYY) - czƒôste w Polsce
    parsed2 = pd.to_datetime(non_null, errors="coerce", dayfirst=True, utc=False)
    success2 = parsed2.notna().mean()
    
    # Wybierz lepszy wynik
    best_success = max(success1, success2)
    
    if best_success >= 0.9:
        return (parsed1 if success1 >= success2 else parsed2), best_success
    else:
        return None, best_success
```

**Obs≈Çugiwane formaty dat:**

```python
# ISO format (YYYY-MM-DD)
"2024-01-15"
"2024-01-15 14:30:00"

# Day-first (DD-MM-YYYY) - polski standard
"15-01-2024"
"15.01.2024"
"15/01/2024"

# Month-first (MM-DD-YYYY) - USA
"01-15-2024"

# Tekstowe
"15 stycznia 2024"  # pandas potrafi!
"Jan 15, 2024"
```

**Pr√≥g sukcesu: 90%**
- Podobnie jak dla liczb
- Je≈õli ‚â•90% to data, je≈õli nie ‚Üí tekst

---

### 4.4 Detekcja kod√≥w (special case)

#### Kod: `app/services/csv_profile.py`

```python
def _is_probably_code_series(s: pd.Series) -> bool:
    """Czy kolumna to kody? (np. kody pocztowe, ID z zerami wiodƒÖcymi)"""
    
    non_null = s.dropna().astype(str).str.strip()
    
    # Case 1: Zera wiodƒÖce (np. "02", "007")
    if (non_null.str.match(r"^0\d+$")).any():
        return True
    
    # Case 2: Wszystkie cyfry, kr√≥tkie, sta≈Çej d≈Çugo≈õci
    # (typowe dla kod√≥w: 1-99, 001-999)
    if non_null.str.match(r"^\d+$").all():
        lengths = non_null.str.len()
        if lengths.max() <= 6 and lengths.nunique() <= 3:
            return True
    
    return False
```

**Dlaczego to wa≈ºne?**

```csv
# Kody pocztowe
Code,City
02-495,Warszawa  # zero wiodƒÖce!
30-001,Krak√≥w
```

Gdyby≈õmy traktowali to jako liczby:
- `"02-495"` ‚Üí b≈ÇƒÖd (my≈õlnik)
- `"02"` ‚Üí `2` (utrata zera!)

**RozwiƒÖzanie:** Zostawiamy jako tekst (categorical)

---

### 4.5 Uzupe≈Çnianie brak√≥w danych

#### Kod: `app/main/processing.py`

```python
def basic_prepare_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """Przygotowanie danych do wizualizacji - uzupe≈Çnianie brak√≥w."""
    
    df = df.copy()
    
    # Kolumny numeryczne ‚Üí uzupe≈Çnij medianƒÖ
    numeric_cols = df.select_dtypes(include="number").columns
    for col in numeric_cols:
        if df[col].isna().any():
            median_value = df[col].median()
            df[col] = df[col].fillna(median_value)
    
    # Kolumny tekstowe ‚Üí uzupe≈Çnij "Brak danych"
    categorical_cols = df.select_dtypes(exclude="number").columns
    for col in categorical_cols:
        if df[col].isna().any():
            df[col] = df[col].fillna("Brak danych")
    
    return df
```

**Strategia uzupe≈Çniania:**

| Typ kolumny | Brak danych | Uzupe≈Çniamy |
|-------------|-------------|-------------|
| Numeryczna | `NaN` | **Mediana** |
| Kategoryczna | `NaN` | **"Brak danych"** |
| Data | `NaT` | (nie uzupe≈Çniamy) |

**Dlaczego mediana, a nie ≈õrednia?**

```python
# Przyk≈Çad: ceny mieszka≈Ñ
prices = [200k, 220k, 210k, 205k, 10M]  # jeden outlier!

mean(prices)   = 2.167M  # z≈Ça reprezentacja!
median(prices) = 210k    # dobra reprezentacja ‚úÖ
```

Mediana jest **odporna na outliery** (warto≈õci odstajƒÖce).

---

## üìà Etap 5: Profilowanie kolumn

### Kod: `app/services/csv_profile.py`

```python
def profile_csv_upload(file_obj) -> CsvProfileResult:
    """G≈Ç√≥wna funkcja - profilowanie przes≈Çanego CSV."""
    
    # ... (parsowanie, detekcja formatu)
    
    dimensions = []     # kolumny kategoryczne
    measures = []       # kolumny numeryczne
    datetimes = []      # kolumny dat
    
    for col in df.columns:
        s = df[col]
        
        # Sprawd≈∫ czy kod
        if _is_probably_code_series(s):
            inferred_type = "string"
            semantic_type = "categorical"
            dimensions.append(col)
        
        else:
            # Pr√≥ba: data
            parsed_dt, dt_ratio = _try_parse_datetime(s)
            if parsed_dt is not None:
                inferred_type = "date"
                semantic_type = "datetime"
                datetimes.append(col)
            
            else:
                # Pr√≥ba: liczba
                parsed_num, num_ratio = _try_parse_numeric(s)
                if parsed_num is not None:
                    inferred_type = "numeric"
                    semantic_type = "measure"
                    measures.append(col)
                
                else:
                    # Fallback: tekst
                    inferred_type = "string"
                    semantic_type = "categorical"
                    dimensions.append(col)
        
        # Statystyki kolumny
        schema_cols.append({
            "name": col,
            "inferred_type": inferred_type,
            "semantic_type": semantic_type,
            "missing_count": int(s.isna().sum()),
            "unique_count": int(s.dropna().nunique()),
            # ... wiƒôcej metadanych
        })
    
    return CsvProfileResult(
        meta={...},
        schema={
            "columns": schema_cols,
            "suggestions": {
                "dimensions": dimensions,    # do group-by
                "measures": measures,         # do agregacji
                "datetimes": datetimes       # do osi czasu
            }
        },
        preview={...}
    )
```

**Wynik profilowania:**

```json
{
  "schema": {
    "columns": [
      {
        "name": "Product",
        "inferred_type": "string",
        "semantic_type": "categorical",
        "missing_count": 2,
        "unique_count": 5
      },
      {
        "name": "Price",
        "inferred_type": "numeric",
        "semantic_type": "measure",
        "missing_count": 1,
        "unique_count": 15,
        "stats": {"min": 10.5, "max": 99.9, "mean": 45.2}
      },
      {
        "name": "Date",
        "inferred_type": "date",
        "semantic_type": "datetime",
        "missing_count": 0,
        "unique_count": 10
      }
    ],
    "suggestions": {
      "dimensions": ["Product", "Category"],
      "measures": ["Price", "Quantity"],
      "datetimes": ["Date"]
    }
  }
}
```

**Zastosowanie:**
- Frontend wie, kt√≥re kolumny mo≈ºna sumowaƒá
- Frontend wie, kt√≥re kolumny pokazaƒá w group-by
- Automatyczne sugestie wykres√≥w

---

## üìä Etap 6: Przygotowanie do wizualizacji

### 6.1 Statystyki

#### Kod: `app/main/processing.py`

```python
def compute_statistics(df: pd.DataFrame) -> Dict[str, Any]:
    """Oblicza statystyki dla ca≈Çego datasetu."""
    
    rows, cols = df.shape
    missing_total = int(df.isna().sum().sum())
    
    numeric_columns = []
    numeric_summary = {}
    
    for col in df.columns:
        series = df[col]
        coerced = pd.to_numeric(series, errors="coerce")
        
        if coerced.notna().any():
            numeric_columns.append(col)
            
            numeric_summary[col] = {
                "min": float(coerced.min()),
                "max": float(coerced.max()),
                "mean": float(coerced.mean()),
                "missing": int(coerced.isna().sum()),
            }
    
    return {
        "rows": int(rows),
        "cols": int(cols),
        "columns": list(df.columns),
        "missing_total": missing_total,
        "numeric_columns": numeric_columns,
        "numeric_summary": numeric_summary,
    }
```

**Przyk≈Çad wyniku:**

```json
{
  "rows": 100,
  "cols": 5,
  "columns": ["Product", "Price", "Quantity", "Date", "City"],
  "missing_total": 12,
  "numeric_columns": ["Price", "Quantity"],
  "numeric_summary": {
    "Price": {
      "min": 10.5,
      "max": 99.9,
      "mean": 45.23,
      "missing": 3
    },
    "Quantity": {
      "min": 1.0,
      "max": 100.0,
      "mean": 25.5,
      "missing": 5
    }
  }
}
```

---

### 6.2 Filtrowanie danych

#### Kod: `app/main/plotly_charts.py`

```python
def _apply_filters(
    df: pd.DataFrame,
    filter_column: str | None = None,
    filter_min: float | None = None,
    filter_max: float | None = None,
    filter_values: list[str] | None = None,
    filter_op: str | None = None,
    filter_value: str | None = None,
) -> pd.DataFrame:
    """Filtruje dane przed wizualizacjƒÖ."""
    
    if not filter_column:
        return df
    
    s = df[filter_column]
    
    # Filtr numeryczny: zakres
    if pd.api.types.is_numeric_dtype(s):
        out = df
        if filter_min is not None:
            out = out[out[filter_column] >= filter_min]
        if filter_max is not None:
            out = out[out[filter_column] <= filter_max]
        return out
    
    # Filtr kategoryczny: lista warto≈õci
    if filter_values:
        return df[df[filter_column].astype(str).isin(filter_values)]
    
    # Filtr tekstowy: contains/equals
    if filter_op and filter_value:
        st = s.astype(str)
        if filter_op == "contains":
            return df[st.str.contains(filter_value, case=False, na=False)]
        if filter_op == "equals":
            return df[st == filter_value]
    
    return df
```

**Przyk≈Çady u≈ºycia:**

```python
# Filtr numeryczny
df_filtered = _apply_filters(
    df,
    filter_column="Price",
    filter_min=20.0,
    filter_max=50.0
)
# Wynik: tylko produkty 20-50 PLN

# Filtr kategoryczny
df_filtered = _apply_filters(
    df,
    filter_column="City",
    filter_values=["Warszawa", "Krak√≥w"]
)
# Wynik: tylko 2 miasta

# Filtr tekstowy
df_filtered = _apply_filters(
    df,
    filter_column="Product",
    filter_op="contains",
    filter_value="laptop"
)
# Wynik: produkty zawierajƒÖce "laptop"
```

---

### 6.3 Generowanie wykres√≥w

#### Kod: `app/main/plotly_charts.py`

```python
def bar_counts(
    df: pd.DataFrame,
    column: str,
    top_n: int = 20,
    **filters
) -> dict:
    """Wykres s≈Çupkowy liczno≈õci kategorii."""
    
    # 1. Filtruj dane
    df_f = _apply_filters(df, **filters)
    
    # 2. Policz warto≈õci
    counts = (df_f[column]
              .astype(str)
              .value_counts(dropna=False)
              .head(top_n)
              .sort_values(ascending=True))  # poziomy wykres
    
    # 3. Przygotuj DataFrame
    cdf = counts.reset_index()
    cdf.columns = [column, "count"]
    
    # 4. Generuj wykres Plotly
    fig = px.bar(
        cdf,
        x="count",
        y=column,
        orientation="h",  # horizontal
        title=f"Liczno≈õci: {column} (top {top_n})",
    )
    
    # 5. Zwr√≥ƒá jako JSON
    return json.loads(pio.to_json(fig))
```

**Inne wykresy:**
- `histogram()` - rozk≈Çad warto≈õci numerycznych
- `boxplot()` - kwartyle, outliery
- `scatter()` - korelacja x vs y
- `corr_heatmap()` - macierz korelacji

---

## üêõ Problemy i rozwiƒÖzania

### Problem 1: Polskie znaki (ƒÖ, ƒá, ƒô...)

**Symptom:**
```
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb1
```

**Przyczyna:**
- Windows Excel zapisuje w CP1250
- Python domy≈õlnie pr√≥buje UTF-8

**RozwiƒÖzanie:**
```python
def _detect_encoding(data: bytes) -> str:
    # Pr√≥ba UTF-8
    try:
        data.decode("utf-8")
        return "utf-8"
    except UnicodeDecodeError:
        return "cp1250"  # fallback
```

---

### Problem 2: ≈örednik jako separator

**Symptom:**
```csv
Name;Age;City
Jan;25;Warszawa
```
Pandas traktuje jako 1 kolumnƒô: `"Name;Age;City"`

**Przyczyna:**
- W Polsce Excel u≈ºywa `;` (bo `,` to separator dziesiƒôtny)
- pandas domy≈õlnie zak≈Çada `,`

**RozwiƒÖzanie:**
```python
# Automatyczna detekcja
delimiter = _detect_delimiter(text_sample)
df = pd.read_csv(file, sep=delimiter)
```

---

### Problem 3: Przecinek dziesiƒôtny

**Symptom:**
```csv
Price
10,5   # chcemy 10.5, nie tekst "10,5"
```

**Przyczyna:**
- Polski standard: `10,5`
- Python/pandas oczekuje: `10.5`

**RozwiƒÖzanie:**
```python
normalized = s.str.replace(",", ".", regex=False)
parsed = pd.to_numeric(normalized, errors="coerce")
```

---

### Problem 4: Zera wiodƒÖce w kodach

**Symptom:**
```csv
PostalCode
02-495    # po konwersji na int: 2495 (utrata zera!)
```

**RozwiƒÖzanie:**
```python
if _is_probably_code_series(s):
    # Zostaw jako string
    inferred_type = "string"
```

---

### Problem 5: R√≥≈ºne reprezentacje brakujƒÖcych danych

**Symptom:**
```csv
Name,Age
Jan,25
Anna,      # pusty
Piotr,null # tekst "null"
Maria,NA   # tekst "NA"
```

**RozwiƒÖzanie:**
```python
# Normalizacja wszystkich ‚Üí pd.NA
df = df.applymap(
    lambda x: pd.NA if (x is None or 
                        (isinstance(x, str) and x.strip() == ""))
              else x
)
```

---

### Problem 6: Duplikaty nazw kolumn

**Symptom:**
```csv
Name,Age,Name    # duplikat!
Jan,25,Kowalski
```

**RozwiƒÖzanie:**
```python
# Automatyczne dodanie sufiks√≥w
seen = {}
new_cols = []
for c in cols:
    if c not in seen:
        seen[c] = 1
        new_cols.append(c)
    else:
        seen[c] += 1
        new_cols.append(f"{c}__{seen[c]}")  # Name__2

df.columns = new_cols
```

---

## üí° Przyk≈Çady end-to-end

### Przyk≈Çad 1: Plik z polskimi znakami

**Input: `sprzedaz.csv` (CP1250, ≈õrednik)**
```csv
Produkt;Cena;Ilo≈õƒá;Miasto
Laptop;2500,50;10;Warszawa
Mysz;45,99;100;Krak√≥w
Klawiatura;;50;Gda≈Ñsk
```

**Pipeline:**
1. ‚úÖ Detekcja: CP1250, separator=`;`
2. ‚úÖ Parsowanie: 4 kolumny, 3 wiersze
3. ‚úÖ Czyszczenie:
   - `"2500,50"` ‚Üí `2500.5`
   - `""` (pusta cena) ‚Üí `pd.NA`
   - `"Gda≈Ñsk"` (polskie znaki) ‚Üí OK
4. ‚úÖ Profilowanie:
   - `Produkt`: categorical
   - `Cena`: numeric (brak: 1)
   - `Ilo≈õƒá`: numeric
   - `Miasto`: categorical
5. ‚úÖ Uzupe≈Çnianie:
   - Cena: `pd.NA` ‚Üí `1273.245` (mediana)
6. ‚úÖ Wizualizacja: wykresy gotowe!

---

### Przyk≈Çad 2: Plik z datami

**Input: `zamowienia.csv`**
```csv
Data,Produkt,Warto≈õƒá
2024-01-15,Laptop,2500
15.01.2024,Mysz,45.99
2024-01-16,Klawiatura,199
```

**Pipeline:**
1. ‚úÖ Detekcja dat:
   - Pr√≥ba 1 (ISO): 66% sukces
   - Pr√≥ba 2 (day-first): 100% sukces ‚úÖ
2. ‚úÖ Konwersja:
   - `"2024-01-15"` ‚Üí datetime
   - `"15.01.2024"` ‚Üí datetime (ten sam!)
3. ‚úÖ Profilowanie:
   - `Data`: datetime
   - `Produkt`: categorical
   - `Warto≈õƒá`: numeric
4. ‚úÖ Sugestia: wykres liniowy (o≈õ X = Data)

---

### Przyk≈Çad 3: Plik z kodami pocztowymi

**Input: `adresy.csv`**
```csv
KodPocztowy,Miasto
02-495,Warszawa
30-001,Krak√≥w
01,Warszawa
```

**Bez detekcji kod√≥w:**
- `"02-495"` ‚Üí b≈ÇƒÖd parsowania (my≈õlnik)
- `"01"` ‚Üí `1` (utrata zera!)

**Z detekcjƒÖ kod√≥w:**
1. ‚úÖ `_is_probably_code_series()` wykrywa zera wiodƒÖce
2. ‚úÖ Kolumna traktowana jako `string`
3. ‚úÖ Warto≈õci zachowane: `"02-495"`, `"01"`

---

## üìö Biblioteki i narzƒôdzia

### pandas
```python
import pandas as pd

# Core operations
pd.read_csv()           # parsowanie
pd.to_numeric()         # konwersja liczb
pd.to_datetime()        # konwersja dat
df.fillna()             # uzupe≈Çnianie brak√≥w
df.isna()               # detekcja brak√≥w
df.select_dtypes()      # filtrowanie po typie
```

### Plotly
```python
import plotly.express as px
import plotly.io as pio

# Wykresy
px.bar()                # wykres s≈Çupkowy
px.histogram()          # histogram
px.box()                # boxplot
px.scatter()            # scatter plot
px.imshow()             # heatmapa

# Serializacja
pio.to_json()           # figura ‚Üí JSON
```

### Python stdlib
```python
import csv

csv.Sniffer()           # detekcja separatora
.sniff()                # detekcja dialektu
.has_header()           # czy ma nag≈Ç√≥wki
```

---

## üéØ Podsumowanie procesu

### Co robimy dobrze ‚úÖ

1. **Automatyczna detekcja formatu**
   - Kodowanie, separator, nag≈Ç√≥wki
   - Dzia≈Ça z r√≥≈ºnymi ≈∫r√≥d≈Çami

2. **Robustne parsowanie**
   - Obs≈Çuga polskich znak√≥w
   - Obs≈Çuga r√≥≈ºnych separator√≥w
   - Graceful degradation

3. **Inteligentna detekcja typ√≥w**
   - Liczby (z przecinkiem)
   - Daty (r√≥≈ºne formaty)
   - Kody (zachowanie zer)

4. **Sensowne czyszczenie**
   - Mediana dla liczb (odporna na outliery)
   - "Brak danych" dla tekstu
   - Normalizacja brak√≥w

5. **Metadane dla frontendu**
   - Sugestie kolumn do wykres√≥w
   - Statystyki
   - PodglƒÖd danych

### Co mo≈ºna poprawiƒá üí°

1. **Wiƒôcej strategii uzupe≈Çniania**
   - Interpolacja dla szereg√≥w czasowych
   - KNN imputation
   - Forward/backward fill

2. **Detekcja outlier√≥w**
   - Z-score
   - IQR method
   - Isolation Forest

3. **Walidacja logiczna**
   - Wiek >0 i <150
   - Cena >0
   - Data nie w przysz≈Ço≈õci

4. **Profilowanie zaawansowane**
   - Korelacje miƒôdzy kolumnami
   - Analiza rozk≈Çad√≥w
   - Wykrywanie anomalii

---

**Dokument stworzony:** 28 stycznia 2026  
**Wersja:** 1.0  
**Autorzy:** Zesp√≥≈Ç WSB Data Charts

